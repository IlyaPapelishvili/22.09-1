{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "klGNgWREsvQv"
      },
      "source": [
        "**Copyright 20201 Die Autoren der TF-Agenten.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
        },
        "colab_type": "code",
        "id": "nQnmcm0oI1Q-"
      },
      "outputs": [

      ],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lsaQlK8fFQqH"
      },
      "source": [
        "# SAC-Minitaurus mit der Actor-Learner-API\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/agents/tutorials/7_SAC_minitaur_tutorial\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ansicht auf TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/agents/blob/master/docs/tutorials/7_SAC_minitaur_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Führen Sie in Google Colab aus</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/agents/blob/master/docs/tutorials/7_SAC_minitaur_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Quelle auf GitHub anzeigen</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/agents/docs/tutorials/7_SAC_minitaur_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Notizbuch herunterladen</a></td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZOUOQOrFs3zn"
      },
      "source": [
        "## Einführung"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cKOCZlhUgXVK"
      },
      "source": [
        "This example shows how to train a [Soft Actor Critic](https://arxiv.org/abs/1812.05905) agent on the [Minitaur](https://github.com/bulletphysics/bullet3/blob/master/examples/pybullet/gym/pybullet_envs/bullet/minitaur.py) environment.\n",
        "\n",
        "Wenn Sie das [DQN Colab durchgearbeitet](https://github.com/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb) haben, sollte sich dies sehr vertraut anfühlen. Bemerkenswerte Änderungen umfassen:\n",
        "\n",
        "- Ändern des Agenten von DQN zu SAC.\n",
        "- Training auf Minitaur, einer viel komplexeren Umgebung als CartPole. Die Minitaur-Umgebung zielt darauf ab, einen Vierbeiner zu trainieren, um sich vorwärts zu bewegen.\n",
        "- Verwenden der TF-Agents Actor-Learner-API für verteiltes Reinforcement-Lernen.\n",
        "\n",
        "Die API unterstützt sowohl die verteilte Datenerfassung mithilfe eines Erfahrungswiedergabepuffers und eines variablen Containers (Parameterserver) als auch das verteilte Training auf mehrere Geräte. Die API ist sehr einfach und modular aufgebaut. Wir verwenden [Reverb](https://deepmind.com/research/open-source/Reverb) sowohl für den Wiedergabepuffer als auch für den variablen Container und die [TF DistributionStrategy-API](https://www.tensorflow.org/guide/distributed_training) für verteiltes Training auf GPUs und TPUs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9vUQms4DAY5A"
      },
      "source": [
        "Wenn Sie die folgenden Abhängigkeiten nicht installiert haben, führen Sie Folgendes aus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "fskoLlB-AZ9j"
      },
      "outputs": [

      ],
      "source": [
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip install 'gym==0.10.11'\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install matplotlib\n",
        "!pip install PILLOW\n",
        "!pip install tf-agents[reverb]\n",
        "!pip install 'pybullet==2.4.2'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1u9QVVsShC9X"
      },
      "source": [
        "## Konfiguration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nNV5wyH3dyMl"
      },
      "source": [
        "Zuerst importieren wir die verschiedenen Tools, die wir benötigen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "sMitx5qSgJk1"
      },
      "outputs": [

      ],
      "source": [
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import reverb\n",
        "import tempfile\n",
        "import PIL.Image\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.ddpg import critic_network\n",
        "from tf_agents.agents.sac import sac_agent\n",
        "from tf_agents.agents.sac import tanh_normal_projection_network\n",
        "from tf_agents.environments import suite_pybullet\n",
        "from tf_agents.experimental.train import actor\n",
        "from tf_agents.experimental.train import learner\n",
        "from tf_agents.experimental.train import triggers\n",
        "from tf_agents.experimental.train.utils import spec_utils\n",
        "from tf_agents.experimental.train.utils import strategy_utils\n",
        "from tf_agents.experimental.train.utils import train_utils\n",
        "from tf_agents.metrics import py_metrics\n",
        "from tf_agents.networks import actor_distribution_network\n",
        "from tf_agents.policies import greedy_policy\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_py_policy\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "\n",
        "tempdir = tempfile.gettempdir()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LmC0NDhdLIKY"
      },
      "source": [
        "## Hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "HC1kNrOsLSIZ"
      },
      "outputs": [

      ],
      "source": [
        "env_name = \"MinitaurBulletEnv-v0\" # @param {type:\"string\"}\n",
        "\n",
        "# Use \"num_iterations = 1e6\" for better results (2 hrs)\n",
        "# 1e5 is just so this doesn't take too long (1 hr)\n",
        "num_iterations = 100000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 10000 # @param {type:\"integer\"}\n",
        "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
        "replay_buffer_capacity = 10000 # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 256 # @param {type:\"integer\"}\n",
        "\n",
        "critic_learning_rate = 3e-4 # @param {type:\"number\"}\n",
        "actor_learning_rate = 3e-4 # @param {type:\"number\"}\n",
        "alpha_learning_rate = 3e-4 # @param {type:\"number\"}\n",
        "target_update_tau = 0.005 # @param {type:\"number\"}\n",
        "target_update_period = 1 # @param {type:\"number\"}\n",
        "gamma = 0.99 # @param {type:\"number\"}\n",
        "reward_scale_factor = 1.0 # @param {type:\"number\"}\n",
        "\n",
        "actor_fc_layer_params = (256, 256)\n",
        "critic_joint_fc_layer_params = (256, 256)\n",
        "\n",
        "log_interval = 5000 # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 20 # @param {type:\"integer\"}\n",
        "eval_interval = 10000 # @param {type:\"integer\"}\n",
        "\n",
        "policy_save_interval = 5000 # @param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VMsJC3DEgI0x"
      },
      "source": [
        "## Umgebung\n",
        "\n",
        "Umgebungen in RL stellen die Aufgabe oder das Problem dar, die wir lösen möchten. Standardumgebungen können in TF-Agenten mithilfe von `suites` einfach erstellt werden. Wir haben verschiedene `suites` zum Laden von Umgebungen aus Quellen wie OpenAI Gym, Atari, DM Control usw., denen ein String-Umgebungsname zugewiesen wurde.\n",
        "\n",
        "Laden wir nun die Minituar-Umgebung aus der Pybullet-Suite."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "RlO7WIQHu_7D"
      },
      "outputs": [

      ],
      "source": [
        "env = suite_pybullet.load(env_name)\n",
        "env.reset()\n",
        "PIL.Image.fromarray(env.render())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gY179d1xlmoM"
      },
      "source": [
        "In dieser Umgebung besteht das Ziel darin, dass der Agent eine Richtlinie trainiert, die den Minitaur-Roboter steuert und ihn so schnell wie möglich vorwärts bringt. Episoden dauern 1000 Schritte und die Rückkehr ist die Summe der Belohnungen während der gesamten Episode.\n",
        "\n",
        "Let's look at the information the environment provides as an `observation` which the policy will use to generate `actions`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "exDv57iHfwQV"
      },
      "outputs": [

      ],
      "source": [
        "print('Observation Spec:')\n",
        "print(env.time_step_spec().observation)\n",
        "print('Action Spec:')\n",
        "print(env.action_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wg5ysVTnctIm"
      },
      "source": [
        "Wie wir sehen können, ist die Beobachtung ziemlich komplex. Wir erhalten 28 Werte, die die Winkel, Geschwindigkeiten und Drehmomente für alle Motoren darstellen. Im Gegenzug erwartet die Umgebung 8 Werte für die Aktionen zwischen `[-1, 1]` . Dies sind die gewünschten Motorwinkel.\n",
        "\n",
        "Normalerweise erstellen wir zwei Umgebungen: eine zum Sammeln von Daten während des Trainings und eine zum Auswerten. Die Umgebungen sind in reinem Python geschrieben und verwenden Numpy-Arrays, die von der Actor Learner-API direkt verwendet werden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "Xp-Y4mD6eDhF"
      },
      "outputs": [

      ],
      "source": [
        "collect_env = suite_pybullet.load(env_name)\n",
        "eval_env = suite_pybullet.load(env_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Da-z2yF66FR9"
      },
      "source": [
        "## Vertriebsstrategie\n",
        "\n",
        "Wir verwenden die DistributionStrategy-API, um die Ausführung der Zugschrittberechnung über mehrere Geräte wie mehrere GPUs oder TPUs mithilfe von Datenparallelität auszuführen. Der Zugschritt:\n",
        "\n",
        "- Erhält eine Reihe von Trainingsdaten\n",
        "- Teilt es auf die Geräte auf\n",
        "- Berechnet den Vorwärtsschritt\n",
        "- Aggregiert und berechnet die BEDEUTUNG des Verlusts\n",
        "- Berechnet den Rückwärtsschritt und führt eine Aktualisierung der Gradientenvariablen durch\n",
        "\n",
        "Mit der TF-Agents Learner API und der DistributionStrategy API können Sie ganz einfach zwischen dem Ausführen des Zugschritts auf GPUs (mit MirroredStrategy) und TPUs (mit TPUStrategy) wechseln, ohne die unten stehende Trainingslogik zu ändern."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wGREYZCaDB1h"
      },
      "source": [
        "### Aktivieren der GPU\n",
        "\n",
        "Wenn Sie versuchen möchten, auf einer GPU ausgeführt zu werden, müssen Sie zuerst die GPUs für das Notebook aktivieren:\n",
        "\n",
        "Navigieren Sie zu Bearbeiten → Notebook-Einstellungen, und wählen Sie in der Dropdown-Liste Hardwarebeschleuniger die Option GPU aus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5ZuvwDV66Mn1"
      },
      "source": [
        "### Eine Strategie auswählen\n",
        "\n",
        "Verwenden Sie `strategy_utils` , um eine Strategie zu generieren. Übergeben Sie unter der Haube den Parameter:\n",
        "\n",
        "- `use_gpu = False` gibt `tf.distribute.get_strategy()` , das die CPU verwendet\n",
        "- `use_gpu = True` gibt `tf.distribute.MirroredStrategy()` , das alle GPUs verwendet, die für TensorFlow auf einem Computer sichtbar sind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "ff5ZZRZI15ds"
      },
      "outputs": [

      ],
      "source": [
        "use_gpu = True #@param {type:\"boolean\"}\n",
        "\n",
        "strategy = strategy_utils.get_strategy(tpu=False, use_gpu=use_gpu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fMn5FTs5kHvt"
      },
      "source": [
        "Alle Variablen und Agenten müssen unter `strategy.scope()` , wie Sie unten sehen werden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E9lW_OZYFR8A"
      },
      "source": [
        "## Agent\n",
        "\n",
        "Um einen SAC-Agenten zu erstellen, müssen wir zuerst die Netzwerke erstellen, die er trainieren wird. SAC ist ein Schauspieler-Kritiker-Agent, daher brauchen wir zwei Netzwerke.\n",
        "\n",
        "Der Kritiker wird uns Wertschätzungen für `Q(s,a)` . Das heißt, es wird als Eingabe eine Beobachtung und eine Handlung erhalten, und es wird uns eine Schätzung geben, wie gut diese Handlung für den gegebenen Zustand war.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "TgkdEPg_muzV"
      },
      "outputs": [

      ],
      "source": [
        "observation_spec, action_spec, time_step_spec = (\n",
        "      spec_utils.get_tensor_specs(collect_env))\n",
        "\n",
        "with strategy.scope():\n",
        "  critic_net = critic_network.CriticNetwork(\n",
        "        (observation_spec, action_spec),\n",
        "        observation_fc_layer_params=None,\n",
        "        action_fc_layer_params=None,\n",
        "        joint_fc_layer_params=critic_joint_fc_layer_params,\n",
        "        kernel_initializer='glorot_uniform',\n",
        "        last_kernel_initializer='glorot_uniform')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pYy4AH4V7Ph4"
      },
      "source": [
        "Wir werden diese Kritiker verwenden , um einen Zug `actor` - Netzwerk , das uns Aktionen eine Beobachtung gegeben generieren können.\n",
        "\n",
        "Das `ActorNetwork` Parameter für eine tanh-gequetschte [MultivariateNormalDiag-](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MultivariateNormalDiag) Verteilung voraus. Diese Verteilung wird dann unter Berücksichtigung der aktuellen Beobachtung abgetastet, wann immer wir Aktionen generieren müssen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "TB5Y3Oub4u7f"
      },
      "outputs": [

      ],
      "source": [
        "with strategy.scope():\n",
        "  actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
        "      observation_spec,\n",
        "      action_spec,\n",
        "      fc_layer_params=actor_fc_layer_params,\n",
        "      continuous_projection_net=(\n",
        "          tanh_normal_projection_network.TanhNormalProjectionNetwork))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z62u55hSmviJ"
      },
      "source": [
        "Mit diesen Netzwerken können wir jetzt den Agenten instanziieren.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "jbY4yrjTEyc9"
      },
      "outputs": [

      ],
      "source": [
        "with strategy.scope():\n",
        "  train_step = train_utils.create_train_step()\n",
        "\n",
        "  tf_agent = sac_agent.SacAgent(\n",
        "        time_step_spec,\n",
        "        action_spec,\n",
        "        actor_network=actor_net,\n",
        "        critic_network=critic_net,\n",
        "        actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
        "            learning_rate=actor_learning_rate),\n",
        "        critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
        "            learning_rate=critic_learning_rate),\n",
        "        alpha_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
        "            learning_rate=alpha_learning_rate),\n",
        "        target_update_tau=target_update_tau,\n",
        "        target_update_period=target_update_period,\n",
        "        td_errors_loss_fn=tf.math.squared_difference,\n",
        "        gamma=gamma,\n",
        "        reward_scale_factor=reward_scale_factor,\n",
        "        train_step_counter=train_step)\n",
        "\n",
        "  tf_agent.initialize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NLva6g2jdWgr"
      },
      "source": [
        "## Puffer wiedergeben\n",
        "\n",
        "Um den Überblick über die aus der Umgebung gesammelten Daten zu behalten, verwenden wir [Reverb](https://deepmind.com/research/open-source/Reverb) , ein effizientes, erweiterbares und benutzerfreundliches Wiedergabesystem von Deepmind. Es speichert Erfahrungsdaten, die von den Schauspielern gesammelt und vom Lernenden während des Trainings konsumiert wurden.\n",
        "\n",
        "In diesem Lernprogramm ist dies weniger wichtig als `max_size` In einer verteilten Umgebung mit asynchroner Erfassung und Schulung möchten Sie wahrscheinlich mit `rate_limiters.SampleToInsertRatio` experimentieren und dabei ein samples_per_insert zwischen 2 und 1000 verwenden. Beispiel:\n",
        "\n",
        "```\n",
        "rate_limiter=reverb.rate_limiters.SampleToInsertRatio(samples_per_insert=3.0, min_size_to_sample=3, error_buffer=3.0))\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "vX2zGUWJGWAl"
      },
      "outputs": [

      ],
      "source": [
        "table_name = 'uniform_table'\n",
        "table = reverb.Table(\n",
        "    table_name,\n",
        "    max_size=replay_buffer_capacity,\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    remover=reverb.selectors.Fifo(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1))\n",
        "\n",
        "reverb_server = reverb.Server([table])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LRNvAnkO7JK2"
      },
      "source": [
        "The replay buffer is constructed using specs describing the tensors that are to be stored, which can be obtained from the agent using `tf_agent.collect_data_spec`.\n",
        "\n",
        "Da der SAC-Agent sowohl die aktuelle als auch die nächste Beobachtung benötigt, um den Verlust zu berechnen, setzen wir `sequence_length=2` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "xVLUxyUo7HQR"
      },
      "outputs": [

      ],
      "source": [
        "reverb_replay = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "    tf_agent.collect_data_spec,\n",
        "    sequence_length=2,\n",
        "    table_name=table_name,\n",
        "    local_server=reverb_server)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rVD5nQ9ZGo8_"
      },
      "source": [
        "Jetzt generieren wir einen TensorFlow-Datensatz aus dem Reverb-Wiedergabepuffer. Wir werden dies an den Lernenden weitergeben, um Erfahrungen für das Training zu sammeln."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "ba7bilizt_qW"
      },
      "outputs": [

      ],
      "source": [
        "dataset = reverb_replay.as_dataset(\n",
        "      sample_batch_size=batch_size, num_steps=2).prefetch(50)\n",
        "experience_dataset_fn = lambda: dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I0KLrEPwkn5x"
      },
      "source": [
        "## Richtlinien\n",
        "\n",
        "In TF-Agents, policies represent the standard notion of policies in RL: given a `time_step` produce an action or a distribution over actions. The main method is `policy_step = policy.step(time_step)` where `policy_step` is a named tuple `PolicyStep(action, state, info)`.  The `policy_step.action` is the `action` to be applied to the environment, `state` represents the state for stateful (RNN) policies and `info` may contain auxiliary information such as log probabilities of the actions.\n",
        "\n",
        "Agenten enthalten zwei Richtlinien:\n",
        "\n",
        "- `agent.policy` - Die Hauptrichtlinie, die für die Evaluierung und Bereitstellung verwendet wird.\n",
        "- `agent.collect_policy` - Eine zweite Richtlinie, die für die Datenerfassung verwendet wird."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "yq7JE8IwFe0E"
      },
      "outputs": [

      ],
      "source": [
        "tf_eval_policy = tf_agent.policy\n",
        "eval_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
        "  tf_eval_policy, use_tf_function=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "f_A4rZveEQzW"
      },
      "outputs": [

      ],
      "source": [
        "tf_collect_policy = tf_agent.collect_policy\n",
        "collect_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
        "  tf_collect_policy, use_tf_function=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "azkJZ8oaF8uc"
      },
      "source": [
        "Richtlinien können unabhängig von Agenten erstellt werden. Verwenden Sie beispielsweise `tf_agents.policies.random_py_policy` , um eine Richtlinie zu erstellen, die zufällig eine Aktion für jeden Zeitschritt auswählt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "BwY7StuMkuV4"
      },
      "outputs": [

      ],
      "source": [
        "random_policy = random_py_policy.RandomPyPolicy(\n",
        "  collect_env.time_step_spec(), collect_env.action_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l1LMqw60Kuso"
      },
      "source": [
        "## Schauspieler\n",
        "\n",
        "Der Akteur verwaltet die Interaktionen zwischen einer Richtlinie und einer Umgebung.\n",
        "\n",
        "- Die Actor-Komponenten enthalten eine Instanz der Umgebung (als `py_environment` ) und eine Kopie der Richtlinienvariablen.\n",
        "- Jeder Actor-Worker führt eine Folge von Datenerfassungsschritten unter Berücksichtigung der lokalen Werte der Richtlinienvariablen aus.\n",
        "- Variable updates are done explicitly using the variable container client instance in the training script before calling `actor.run()`.\n",
        "- Die beobachtete Erfahrung wird in jedem Datenerfassungsschritt in den Wiedergabepuffer geschrieben."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XjE59ct9fU7W"
      },
      "source": [
        "Während die Akteure Datenerfassungsschritte ausführen, geben sie Trajektorien von (Status, Aktion, Belohnung) an den Beobachter weiter, der sie zwischenspeichert und in das Reverb-Wiedergabesystem schreibt.\n",
        "\n",
        "Wir speichern Trajektorien für Frames [(t0, t1) (t1, t2) (t2, t3), ...], weil `stride_length=1` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "HbyGmdiNfNDc"
      },
      "outputs": [

      ],
      "source": [
        "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "  reverb_replay.py_client,\n",
        "  table_name,\n",
        "  sequence_length=2,\n",
        "  stride_length=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6yaVVC22fOcF"
      },
      "source": [
        "Wir erstellen einen Akteur mit der Zufallsrichtlinie und sammeln Erfahrungen, um den Wiedergabepuffer zu setzen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "ZGq3SY0kKwsa"
      },
      "outputs": [

      ],
      "source": [
        "initial_collect_actor = actor.Actor(\n",
        "  collect_env,\n",
        "  random_policy,\n",
        "  train_step,\n",
        "  steps_per_run=initial_collect_steps,\n",
        "  observers=[rb_observer])\n",
        "initial_collect_actor.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6Pkg-0vZP_Ya"
      },
      "source": [
        "Instanziieren Sie einen Schauspieler mit der Sammlungsrichtlinie, um während des Trainings mehr Erfahrungen zu sammeln."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "A6ooXyk0FZ5j"
      },
      "outputs": [

      ],
      "source": [
        "env_step_metric = py_metrics.EnvironmentSteps()\n",
        "collect_actor = actor.Actor(\n",
        "  collect_env,\n",
        "  collect_policy,\n",
        "  train_step,\n",
        "  steps_per_run=1,\n",
        "  metrics=actor.collect_metrics(10),\n",
        "  summary_dir=os.path.join(tempdir, learner.TRAIN_DIR),\n",
        "  observers=[rb_observer, env_step_metric])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FR9CZ-jfPN2T"
      },
      "source": [
        "Erstellen Sie einen Akteur, mit dem die Richtlinie während des Trainings bewertet wird. Wir übergeben `actor.eval_metrics(num_eval_episodes)` , um Metriken später zu protokollieren."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "vHY2BT5lFhgL"
      },
      "outputs": [

      ],
      "source": [
        "eval_actor = actor.Actor(\n",
        "  eval_env,\n",
        "  eval_policy,\n",
        "  train_step,\n",
        "  episodes_per_run=num_eval_episodes,\n",
        "  metrics=actor.eval_metrics(num_eval_episodes),\n",
        "  summary_dir=os.path.join(tempdir, 'eval'),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y6eBGSYiOf83"
      },
      "source": [
        "## Lernende\n",
        "\n",
        "Die Learner-Komponente enthält den Agenten und führt mithilfe von Erfahrungsdaten aus dem Wiedergabepuffer Gradientenschrittaktualisierungen an den Richtlinienvariablen durch. Nach einem oder mehreren Trainingsschritten kann der Lernende einen neuen Satz variabler Werte in den variablen Container verschieben."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "gi37YicSFTfF"
      },
      "outputs": [

      ],
      "source": [
        "saved_model_dir = os.path.join(tempdir, learner.POLICY_SAVED_MODEL_DIR)\n",
        "\n",
        "# Triggers to save the agent's policy checkpoints.\n",
        "learning_triggers = [\n",
        "    triggers.PolicySavedModelTrigger(\n",
        "        saved_model_dir,\n",
        "        tf_agent,\n",
        "        train_step,\n",
        "        interval=policy_save_interval),\n",
        "    triggers.StepPerSecondLogTrigger(train_step, interval=1000),\n",
        "]\n",
        "\n",
        "agent_learner = learner.Learner(\n",
        "  tempdir,\n",
        "  train_step,\n",
        "  tf_agent,\n",
        "  experience_dataset_fn,\n",
        "  triggers=learning_triggers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "94rCXQtbUbXv"
      },
      "source": [
        "## Metriken und Auswertung\n",
        "\n",
        "Wir haben den eval Actor oben `actor.eval_metrics` instanziiert, wodurch die am häufigsten verwendeten Metriken während der Richtlinienbewertung erstellt werden:\n",
        "\n",
        "- Durchschnittliche Rendite. Die Rendite ist die Summe der Belohnungen, die beim Ausführen einer Richtlinie in einer Umgebung für eine Episode erzielt werden. In der Regel wird dies über einige Episoden gemittelt.\n",
        "- Durchschnittliche Episodenlänge.\n",
        "\n",
        "Wir führen den Akteur aus, um diese Metriken zu generieren."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "83iMSHUC71RG"
      },
      "outputs": [

      ],
      "source": [
        "def get_eval_metrics():\n",
        "  eval_actor.run()\n",
        "  results = {}\n",
        "  for metric in eval_actor.metrics:\n",
        "    results[metric.name] = metric.result()\n",
        "  return results\n",
        "\n",
        "metrics = get_eval_metrics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "jnOMvX_eZvOW"
      },
      "outputs": [

      ],
      "source": [
        "def log_eval_metrics(step, metrics):\n",
        "  eval_results = (', ').join(\n",
        "      '{} = {:.6f}'.format(name, result) for name, result in metrics.items())\n",
        "  print('step = {0}: {1}'.format(step, eval_results))\n",
        "\n",
        "log_eval_metrics(0, metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hWWURm_rXG-f"
      },
      "source": [
        "Überprüfen Sie das [Metrikmodul](https://github.com/tensorflow/agents/blob/master/tf_agents/metrics/tf_metrics.py) auf andere Standardimplementierungen verschiedener Metriken."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hBc9lj9VWWtZ"
      },
      "source": [
        "## Schulung des Agenten\n",
        "\n",
        "Die Trainingsschleife umfasst sowohl das Sammeln von Daten aus der Umgebung als auch das Optimieren der Netzwerke des Agenten. Unterwegs werden wir gelegentlich die Richtlinien des Agenten bewerten, um festzustellen, wie es uns geht."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "0pTbJ3PeyF-u"
      },
      "outputs": [

      ],
      "source": [
        "#@test {\"skip\": true}\n",
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# Reset the train step\n",
        "tf_agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = get_eval_metrics()[\"AverageReturn\"]\n",
        "returns = [avg_return]\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "  # Training.\n",
        "  collect_actor.run()\n",
        "  loss_info = agent_learner.run(iterations=1)\n",
        "\n",
        "  # Evaluating.\n",
        "  step = agent_learner.train_step_numpy\n",
        "\n",
        "  if eval_interval and step % eval_interval == 0:\n",
        "    metrics = get_eval_metrics()\n",
        "    log_eval_metrics(step, metrics)\n",
        "    returns.append(metrics[\"AverageReturn\"])\n",
        "\n",
        "  if log_interval and step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, loss_info.loss.numpy()))\n",
        "\n",
        "rb_observer.close()\n",
        "reverb_server.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "68jNcA_TiJDq"
      },
      "source": [
        "## Visualisierung\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aO-LWCdbbOIC"
      },
      "source": [
        "### Grundstücke\n",
        "\n",
        "Wir können die durchschnittliche Rendite im Vergleich zu globalen Schritten darstellen, um die Leistung unseres Agenten zu sehen. In `Minitaur` basiert die Belohnungsfunktion darauf, wie weit der Minitaur in 1000 Schritten geht und den Energieverbrauch bestraft."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "rXKzyGt72HS8"
      },
      "outputs": [

      ],
      "source": [
        "#@test {\"skip\": true}\n",
        "\n",
        "steps = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(steps, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Step')\n",
        "plt.ylim()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M7-XpPP99Cy7"
      },
      "source": [
        "### Videos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9pGfGxSH32gn"
      },
      "source": [
        "Es ist hilfreich, die Leistung eines Agenten zu visualisieren, indem die Umgebung bei jedem Schritt gerendert wird. Bevor wir das tun, erstellen wir zunächst eine Funktion zum Einbetten von Videos in diese Spalte."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "ULaGr8pvOKbl"
      },
      "outputs": [

      ],
      "source": [
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9c_PH-pX4Pr5"
      },
      "source": [
        "Der folgende Code veranschaulicht die Richtlinien des Agenten für einige Episoden:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "PSgaQN1nXT-h"
      },
      "outputs": [

      ],
      "source": [
        "num_episodes = 3\n",
        "video_filename = 'sac_minitaur.mp4'\n",
        "with imageio.get_writer(video_filename, fps=60) as video:\n",
        "  for _ in range(num_episodes):\n",
        "    time_step = eval_env.reset()\n",
        "    video.append_data(eval_env.render())\n",
        "    while not time_step.is_last():\n",
        "      action_step = eval_actor.policy.action(time_step)\n",
        "      time_step = eval_env.step(action_step.action)\n",
        "      video.append_data(eval_env.render())\n",
        "\n",
        "embed_mp4(video_filename)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [

      ],
      "name": "7_SAC_minitaur_tutorial.ipynb",
      "private_outputs": true,
      "provenance": [

      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
